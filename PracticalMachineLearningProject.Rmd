---
title: "Examining HAR Data for Workout Quality"
author: "Craig Ching"
date: "`r Sys.Date()`"
output: html_document
#output: rmarkdown::html_vignette
#vignette: >
#  %\VignetteIndexEntry{Vignette Title}
#  %\VignetteEngine{knitr::rmarkdown}
#  %\VignetteEncoding{UTF-8}
---

```{r init, echo=FALSE, results='hide', message=FALSE}
library(caret)
library(randomForest)
library(gbm)
library(dplyr)
library(ggplot2)

figures <- c("Figure 1. \"Distribution of the classe variable\"",
             "Figure 2. \"Method for cleaning the data\"",
             "Figure 3. \"Random Forest model\"",
             "Figure 4. \"Error in Random Forest model\"",
             "Figure 5. \"Tree Boosting model\"",
             "Figure 6. \"Error in Tree Boosting model\"")

```

## Executive Summary

We were asked to use the data from the study "Qualitative Activity Recognition of Weight Lifting Exercises" (http://groupware.les.inf.puc-rio.br/har) to reproduce the section from that paper that uses a classification machine learning algorithm to predict the type of mistakes a person might make when performing weight lifting exercises.  The paper concludes that, "even though our results point out that it is possible to detect mistakes by classification, this approach is hardly scalable."

Nevertheless, we will use the data they collected to reproduce some of their results.  In this analysis we show that it is possible to accurately classify the quality of a weight lifting exercise.

## Introduction

This study undertakes a lesser studied aspect of physical activity, the study of whether a participant is performing an activity in a correct and/or an efficient manner.  The study had users perform a set of weight lifting activities correctly and then by performing the activities in a manner incorporating common mistakes.  Sensors were placed on users while performing the activities and activities were supervised by an experienced weight lifter to ensure the activities were carried out correctly.  The activities were:

- (Class A) exactly according to the specification
- (Class B) throwing the elbows to the front
- (Class C) lifting the dumbbell only halfway
- (Class D) lowering the dumbbell only halfway
- (Class E) throwing the hips to the front

The exercises were performed by six males between the ages of 20 and 28 years old.  The goal of this analysis is to use the data provided by the study to predict 20 outcomes for which we do not know the *classe* (A, B, C, D, or E).  The data for each *classe* value are distributed fairly uniformly as can be seen in **`r figures[1]`**, giving us confidence that we have enough data for each *classe* to ensure we at least won't have any bias due to not having enough data for a *classe* value.

```{r get, echo=FALSE, results='hide', message=FALSE}
download <- function(name) {

    prefix <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-"
    url <- paste(prefix, name, ".csv", sep = "")
    destfile <- paste(name, ".csv", sep = "")

    if (! file.exists(destfile)) {
        download.file(url, destfile = destfile, method = "curl")
    }
}

setwd("/Users/cching/Coursera/Practical Machine Learning/PracticalMachineLearning/")

lapply(c("training", "testing"), download)

trainRaw <- read.csv("training.csv", na.strings = c("NA", ""), stringsAsFactors = FALSE)

# Build a data frame for the distribution of the classe variable
df.summary <-
    group_by(trainRaw, classe) %>%
    summarize(counts = n())

ncases <- nrow(trainRaw)
nfeatures <- ncol(trainRaw)

# Summarize the counts of non-NA values in each column.  We use
# this to justify removing columns for which there are no valid
# values (all NA)
counts <- apply(trainRaw, 2, function(x) length(which(!is.na(x))))
df.counts <- as.data.frame(table(counts))

```

## Feature Engineering

The raw data consists of `r nfeatures` features and `r ncases` cases.  We note that the data frame is fairly sparse with `r df.counts[1,2]` features containing only `r df.counts[1,1]` values of `r ncases` cases.  We remove these features as their numbers are a small percentage of the overall cases and, as we will see, our predictive power is good without them.  We also remove some time-based features as well as the user name and the identity column as they shouldn't have any predictive value for a classification problem.  Our cleaning code can be reviewed in **`r figures[2]`.**

```{r clean, echo=FALSE, results='hide', message=FALSE}
clean <- function(data, include = TRUE) {

    # Now remove those columns that have no valid values, leaving
    # only columns that have every row with a value
    df <- data[,colSums(!is.na(data))==nrow(data)]

    # The following are not relevant to the analysis and don't
    # make any sense as predictors
    df$X <- NULL
    df$user_name <- NULL
    df$raw_timestamp_part_1 <- NULL
    df$raw_timestamp_part_2 <- NULL
    df$cvtd_timestamp <- NULL
    df$new_window <- NULL
    df$num_window <- NULL
    if (include) {
        df$classe <- as.factor(df$classe)
    }
    df
}

# Summarize the counts of non-NA values in each column.  We use
# this to justify removing columns for which there are no valid
# values (all NA)
counts <- apply(trainRaw, 2, function(x) length(which(!is.na(x))))
nrow(trainRaw)
table(counts)

trainRaw <- clean(trainRaw)

```

After cleaning the data according to the above criteria, we have narrowed our features down to `r ncol(trainRaw)` relevant features.

## Model Selection

Our strategy for model selection is to split our training data into two sets, 70% for training our model and the other 30% for testing and reporting our expected out of sample error.  Finally we will apply our model to the validation set provided by download from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv which will be submitted for evaluation of our final model.

We will create two different machine learning algorithms, one based on random forests and the other based on boosting trees, and train them on the training set using 3-fold cross validation.  We will measure the accuracy and error for each algorithm using the held out test and compare the two resulting models.

```{r partition, echo=FALSE, results='hide', message=FALSE}
getTrainTest <- function(data, shouldSamp = FALSE) {

    d <- data

    if (shouldSamp) {
        d <- data[sample(nrow(data), size = 1500), ]
    }

    inTrain <- createDataPartition(y = data$classe, p = 0.7, list = FALSE)

    newTrain <- data[inTrain, ]
    newTest  <- data[-inTrain, ]

    list(train = newTrain, test = newTest)
}

set.seed(88888888)
l <- getTrainTest(trainRaw)
train <- l$train
test <- l$test

```

```{r results, echo=FALSE}

analyzeModel <- function(model, test) {

    test.pred <- predict(model, newdata = test)

    # cm <- confusionMatrix(test$class, pred)

    testFinal <- read.csv("testing.csv", na.strings = c("NA", ""), stringsAsFactors = FALSE)
    testFinal <- clean(testFinal, FALSE)

    final.pred <- predict(model, newdata = testFinal)

    list(test.pred = test.pred, final.pred = final.pred)
}
```

#### Random Forest Performance

For the random forest model, we use the caret package to train using "rf" for the method.  How we built the model can be seen in **`r figures[3]`.**

```{r rf, echo=FALSE, results='hide', message=FALSE}
getRandomForestModel <- function(data) {

    # Use the cached model if available
    if (file.exists("rf1")) {
        mod <- readRDS("rf1")
    } else {
        trControl <- trainControl(method = "cv", number = 3)
        mod <- train(classe ~ ., method = "rf", data = data, trControl = trControl, do.trace = FALSE, ntree = 500)
        # Cache the model for subsequent runs
        saveRDS(mod, file = "rf1")
    }

    mod
}

set.seed(88888888)

rf.model <- getRandomForestModel(train)

l <- analyzeModel(rf.model, test)

rf.pred <- l$test.pred
rf.final.pred <- l$final.pred

rf.accuracy <- round(postResample(test$classe, rf.pred)[[1]], 3) * 100
rf.error <- 100 - rf.accuracy

```

The random forest model we built has an accuracy of `r rf.accuracy`% and an out-of-sample error of `r rf.error`%.  The accuracy and out-of-sample error were calculated on the held out test set.  This model has a very high accuracy and should perform well on our validation set for submission.  See **`r figures[4]`** for a breakdown in caret's model selection process for the random forest model we built.  The confusion matrix for our random forest model generated on the test data set:

```{r echo=FALSE}

confusionMatrix(test$classe, rf.pred)

```

#### Tree Boosting Performance

For the tree boosting model, we use the caret package to train using "gbm" for the method.  How we built the model can be seen in **`r figures[5]`.**

```{r boosting, echo=FALSE, results='hide', message=FALSE}

getBoostingModel <- function(data) {

    # Use the cached model if available
    if (file.exists("gbm1")) {
        mod <- readRDS("gbm1")
    } else {
        tuneGrid <- expand.grid(n.trees = seq(1,501,10), interaction.depth = 2:5, shrinkage = 0.1, n.minobsinnode = 10)
        # trControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1, verboseIter = FALSE, returnResamp = "all")
        trControl <- trainControl(method = "cv", number = 3)
        mod <- train(classe ~ ., data = data, method = "gbm", trControl = trControl, tuneGrid = tuneGrid)

        # Cache the model for subsequent runs
        saveRDS(mod, file = "gbm1")
    }
    mod
}

set.seed(88888888)

gbm.model <- getBoostingModel(train)
l <- analyzeModel(gbm.model, test)

gbm.pred <- l$test.pred
gbm.final.pred <- l$final.pred

gbm.accuracy <- round(postResample(test$classe, gbm.pred)[[1]], 3) * 100
gbm.error <- 100 - gbm.accuracy

```

The tree boosting model we built has an accuracy of `r gbm.accuracy`% and an out-of-sample error of `r gbm.error`%.  The accuracy and out-of-sample error were calculated on the held out test set.  This model has a very high accuracy and should perform well on our validation set for submission.  See **`r figures[6]`** for a breakdown in caret's model selection process for the tree boosting model we built.   The confusion matrix for our tree boosting model generated on the test data set:

```{r echo=FALSE}

confusionMatrix(test$classe, gbm.pred)

```

```{r submission, echo=FALSE, results='hide', message=FALSE}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(rf.final.pred)

```

## Final Model

```{r echo=FALSE}
model.matches <- sum(rf.final.pred == gbm.final.pred)
```

Comparing the two models, we find that the models agree with each other when predicting on the final validation set `r model.matches` times, they are in complete agreement with each other.  The final predicted values that we submitted are:

```{r echo=FALSE}
print(rf.final.pred)
```

For our final model, we chose the random forest because it was slightly better than the boosting trees algorithm and it was faster to generate.  Both were equally predictive and either could have been chosen.

## Conclusion

Given the methodology of the experiment, that weight trainers supervised participants while performing the exercises, it is perhaps not surprising that our data provides very strong predictors allowing our predictive models to be highly accurate.  Indeed the real challenge would be to find predictive models on data that is more "noisy" with participants who are not coached to perform the exercises in any particular way.  This is actually the premise of the study, that machine learning models can't scale to be able to make these sorts of predictions.  It would be interesting to collect the more noisy data and compare.

## Appendix

Following are all the figures for this document.  Source to reproduce this document and all the analysis herein can be found at https://github.com/craigching/PracticalMachineLearning

```{r barplot, echo=FALSE, results='hide', message=FALSE, fig.width=6, fig.height=4, fig.cap="Distribution of classe variable"}
ggplot(data = df.summary, aes(x = classe, y = counts)) +
    geom_bar(stat = "identity", color = "black", fill = "steelblue") +
    ggtitle("Distribution of the classe variable") +
    theme_bw()
```

`r figures[1]`.

```{r clean, echo=TRUE, results='hide'}
```

`r figures[2]`.

```{r rf, echo=TRUE, results='hide'}
```

`r figures[3]`.

```{r rf_plot, echo=FALSE}
plot(rf.model$finalModel, main = "Random Forest Error for Number of Trees")
```

`r figures[4]`.

```{r boosting, echo=TRUE, results='hide'}
```

`r figures[5]`.

```{r boosting_plot, echo=FALSE}
plot(gbm.model, main = "Accuracy of Tree Boosting Model over Iterations")
```

`r figures[6]`.
